
---
title: "Machine Learning Project"
author: "Winston Saunders"
date: "Dec 2014"
output: html_document
---

###Summary

Project for the Coursera "Machine Learning" course taught by Roger Peng, Jeff Leek, and Brian Caffo of the Dept of Statistics of the Bloomberg School at Johns Hopkins University.

The data used for this project have been generously provided by [Groupware Brasil](http://groupware.les.inf.puc-rio.br/har#ixzz3M5OnVvGE).  

_Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013._
 
The comprise motion measurements of different subjects doing a weight-lifting activity both correctly and incorrectly. The point of the exercise is to build a model from a test data set. Cross correlate the model to ensure it is not over-trained, and then apply it to make predictions of motion classification on a test data set. 


###A: Getting and cleaning data

```{r, echo=FALSE}

if(getwd()!="/Users/winstonsaunders/Documents/MachineLearningProject") setwd("MachineLearningProject")

##Read the files assigning na.strings to blanks and "NA"
training<-read.csv("pml-training.csv", na.strings=c("", "NA"))
testing<-read.csv("pml-testing.csv", na.strings=c("", "NA"))

```

The raw training set consists of `r dim(training)[1]` rows and `r dim(training)[2]` columns.

```{r "look", eval=FALSE, echo=FALSE}

##look at raw data

head(training,2)

head(testing, 2)

```

Inspection reveals there are many NA values in the data. A historgram shows that there are really two classes of columns. Those with data and those without. To filter them I choose to eliminate those columns with high numbers (arbitrarily picked >50%) of NA values.   
The cleaning also includes steps to eliminate non-data columns like timestamps, etc.

```{r , echo=FALSE}
#number of observations
nObs<-length(training$classe)
```

Here is a look at the somecolumns of the raw data, showing both relevant and non-relevant data.

```{r, echo=7}
##This code chunk clean up the data and creates macro cleaned data sets Ctraining and Ctesting.
        ##eliminate "high NA value" columns
        Ctraining<-training[, colSums(is.na(training))/nObs<.5]
        #eliminate the same columns from the testing data 
        Ctesting<-testing[, colSums(is.na(training))/nObs<.5] 
        ##inspect the Ctraining Dataset
        Ctraining[1:3, c(5, 8, 12, 30, 41)]

##Get rid of non-data columns

        Ctraining<-Ctraining[,8:ncol(Ctraining)]
        Ctesting<-Ctesting[,8:ncol(Ctesting)]

        ##verify no NAs left
        ##colSums(is.na(Ctraining))

```

Cleaning reduces the number of data columns from `r dim(training)[2]` to `r dim(Ctraining)[2]`. The testing data also is reduced to `r dim(Ctesting)[2]` columns as expected.  

###B: Data Exploration

Here we look into the data to understand its behavior. However, since we want to cross validate the data prior to making predictions, I first divide the training set.

####B.1: Dividing the Test Data into a training and test subsets for cross validation  
Use the caret package to create a data partition with 60% of the data in the training set and 40% in the test set.   

```{r, echo=6:8, message=FALSE}
library(caret)
#library(AppliedPredictiveModeling)

        set.seed(8675309)
        ## Divide the Cleaned Training Data into a Train and Test Data set
        inTrain <- createDataPartition(y=Ctraining$classe, p=0.60, list=FALSE) 
        Train <- Ctraining[inTrain,] 
        Test <- Ctraining[-inTrain,]

```


####B.2: Plot a few dependencies to get an idea of what the data look like

I plot a few columns of the data just to see what we are dealing with. In general is it hard to see strong dependencies of any particular variable (though in some cases they are recognized). There are clear differences in some cases between the plots and not in others. 

```{r, echo=FALSE}

par(mfrow=c(2,3))
plot(Train$classe,Train[,2], main=colnames(Train)[2])
plot(Train$classe,Train[,16], main=colnames(Train)[16])
plot(Train$classe,Train[,27], main=colnames(Train)[27])
plot(Train$classe,Train[,31], main=colnames(Train)[31])
plot(Train$classe,Train[,40], main=colnames(Train)[40])
plot(Train$classe,Train[,42], main=colnames(Train)[42])

```

Notice particularly the difference in the mean values for __classe C__ in `r colnames(Train)[40]` and `r colnames(Train)[42]`. We'll look at this again after the model is developed.

###C: Building a Model

So let's attempt a model....

```{r, eval=FALSE, echo=FALSE}
##This is some code I did not use in the final project. Tried to do a PCA on the variables hoping to speed up computation. 
##I could not get it to work but intend to come back to it.

##Analysis to find out how many variables are needed to do describe the data.

        last<-ncol(Train)
        prep_Train<-preProcess(Train[,-last])
        centerscaled<-predict(prep_Train, Train[,-last])
        summary(prcomp(centerscaled))


```


```{r, eval=FALSE, echo=FALSE}
        ##new variables
        pcaprep<-preProcess(Train[,-last], method="pca", pcaComp=10)
        TrainPCA<-predict(pcaprep, Train[,-last])

##Build Random Forest Model
        #Start a timer
        time0 <- proc.time()
        
        modelPCA<-train(Train$classe~., method="rf", data=TrainPCA)

        calctime<-proc.time()-time0

 ##use the model to make a prediction using the raw data
        predictionPCA <- predict(modelPCA, Test[,-last])
        ##test confusion matrix
        confusionMatrix(Test$classe, predictionPCA)


```
Model the training data using the random forest technique. I played around with the parameters a bit to optimize execution. For instance using model type cv seemed to increase speed a bit and produce what I felt was a better confusion matrix. Method "boot" tended to skew the error a bit. I'm unclear on the allow parallel but when I saw it in the documentation I used it since I have a multicore processor.  
I experimented with the number variable. Using _number = 3_ decreased the user time about 25% from _number = 4_, without noticable reduction in accuracy.   

```{r "run model", echo=6:7, message=FALSE}
        ##Run this
        last<-ncol(Train)
        ##understand how long calculation in taking
        time0 <- proc.time()
        ##htraining control parameters and the model
        control1 <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
        model1 <- train(classe~., data = Train, method="rf", trControl = control1, prox=TRUE)

        calctime<-proc.time()-time0
```

A cross validation with the testing data subsetted from the original training data shows the model is very accurate. The confusion matrix is below. 

```{r, echo=c(3,5)}

        ##use the model to make a prediction using the raw data
        prediction1 <- predict(model1, Test)
        ##test confusion matrix
        confusionMatrix(Test$classe, prediction1)

        

        #model2<-train(Train$classe~., method="glm", data=Train)

        #confusionMatrix(Test$classe,predict(modelfit,Test))
```

According to the confusion matrix the model has an accuracy of 99.3% and hence an estimated out of sample error rate of 0.7%.  

For information this calculation took `r calctime[3]` seconds of elapsed time and `r calctime[2]` seconds of system time on a _MacBook Air_. Every time I ran it my battery level dropped about 10%!!!

###D: Looking at points

Below is a graph of pairs of data points with two of the graphs above. 

```{r "add prox plot", echo=FALSE, fig.align='center'}
require(randomForest)
require(caret)

##pick columns
        aa<-40
        bb<-42
##define class centers
        varvis <- classCenter(Train[,c(aa,bb)], Train$classe, model1$finalModel$prox)
        varvis <- as.data.frame(varvis)

varvis$classe <- rownames(varvis)

p <- qplot(Train[,aa], Train[,bb], col=classe, data=Train)
p <- p + geom_point(aes(x=varvis[,1],y=varvis[,2],col=classe),size=3,shape=15,data=varvis)
p <- p + xlab(colnames(Train)[aa]) + ylab(colnames(Train)[bb])


p


```
  
The verical axis is __`r colnames(Train)[aa]`__ and the horizontal axis is __`r colnames(Train)[bb]`__. The broad distribution of points and closeness of the centroids is obvious. Nevertheless they are distinguishable - particularly the location of the __classe C__ centroid. 

With so many variables it is hard to visualize dependencies easily. Nevertheless, this gives an idea of how "noisy" the data appear and yet how powerful the model is at making distinctions. 
  
###E. Using the model to make Predictions

We can use the model to make predictions on the testing data set from the exercise. The model makes all predictions accurately. 

I use the funciton provided in class to create answer files.

```{r "evaluate test data", echo=1}
        answers<-predict(model1, Ctesting)
        
        answers

```

```{r "create file for answers", echo=FALSE}
        ##create function to store answers as text file per instructions
        ##this function came from the class instructions.
        pml_write_files = function(x){
                n = length(x)
                        for(i in 1:n){
                        filename = paste0("problem_id_",i,".txt")
                        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
                        }
                }
 
        ##Write the answers
        pml_write_files(answers)

```

###Conclusions

Using data from the HAR dataset we are able to identify with high accuracy the type of activity undertaken using a [Random Forest](http://en.wikipedia.org/wiki/Random_forest) model. 

The model takes about 8 minutes to run on a laptop computer and is over 99% accurate. 
  
    
